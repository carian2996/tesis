En el presente capítulo presentamos la definición de martingala, así como algunas propiedades que surgen gracias a la estructura de estos procesos. Las martingalas son una de las herramientas más útiles de la teoría de probabilidad moderna. En particular, las martingalas son una referencia para el estudio de teoremas límite o convergencia, pues la teoría de martingalas proporciona pruebas más elegantes y estéticamente atractivas.

\section{Definición}
Consideremos un espacio de probabilidad conocido y fijo $(\Omega, \mathcal{F}, \mathbb{P})$, así como una sucesión de $\sigma$-algebras $(\mathcal{F}_n, n \geq 0)$, tales que $\mathcal{F}_n \subset \mathcal{F}_{n+1} \subset \mathcal{F}$ para todo $n \geq 0$.

\begin{definition}[Martingala]
\label{martingala}
	Decimos que $X = (X_n, n \geq 0)$ es una $(\mathcal{F}_n)$-\emph{martingala}, si
	\begin{enumerate}
		\item $\mathbb{E}[|X_n|] < \infty$ para cada $n$;
		\item $X_n$ es $\mathcal{F}_n$-medible, para cada $n$;
		\item $\mathbb{E}[X_n \mid \mathcal{F}_m] = X_m$ c.s., para todo $m \leq n$.
	\end{enumerate}
\end{definition}

Podemos observar que (2) es \emph{casi} una implicación de (3), la cual sostiene que $X_m$ es casi seguramente igual a una variable aleatoria $\mathcal{F}_m$ medible. \\

Recordemos que la Ley Fuerte de los Grandes Números afirma que \cite[p.~173]{jacodprotter}: si $(X_n, n \geq 0)$ es una sucesión de variables aleatorias las cuales son independientes e idénticamente distribuidas con $\mathbb{E}[X_n] = \mu$ y $\sigma_{X_n}^{2} < \infty$ para toda $n$, y si $S_n = \sum_{j=1}^{n} X_j$, entonces $\lim_{n \rightarrow \infty} \frac{S_n}{n} = \mu$, casi seguramente. Por otro lado, la Ley Cero Uno de Kolmogorov nos asegura que \cite[p.~381]{shiryaev}, si consideramos una sucesión de variables aleatorias $(X_n, n \geq 0)$ independientes entre si, el evento cola debe tener una probabilidad 0 o 1, es decir, debe ser una constante. Entonces, $\frac{S_n}{n} \rightarrow \mu$ cuando $n \rightarrow \infty$, donde $\mu$ es una variable aleatoria cuyo valor siempre es constante. \\

Es de poco interés el estudio de sucesiones convergentes a límites que sean constantes. Sin embargo, si escribimos la sucesión anterior de una manera distinta tenemos que
\begin{align*}
	\lim_{n \rightarrow \infty} \frac{S_n - n\mu}{n} = 0, \hspace{0.3cm} c.s.
\end{align*}

Veamos en el siguiente ejemplo que una propiedad clave de la sucesión $(S_n - n\mu, n \geq 0)$ es que, si $\mathcal{F}_n = \sigma ( S_k \mid k \leq n )$, entonces la sucesión es una martingala.

\begin{example}
	Sea $(X_n, n \geq 0)$ una sucesión de variables aleatorias independientes con $\mathbb{E}[|X_n|] < \infty$, para toda $n$. Para $n \geq 0$ sea $\mathcal{F}_n = \sigma ( S_k \mid k \leq n )$ y $S_n = \sum_{k=0}^n X_k$. Entonces tenemos que para toda $m \leq n$
	\begin{align}
		\mathbb{E}[S_n - n\mu \mid \mathcal{F}_m] & = \mathbb{E}[S_m + (S_n - S_m) - n\mu \mid \mathcal{F}_m] \nonumber \\
		& = (S_m - n\mu) + \mathbb{E}[S_n - S_m \mid \mathcal{F}_m] \nonumber \\
		& = (S_m - n\mu) + \mathbb{E}\left[\sum_{k = m+1}^{n} X_k \mid \mathcal{F}_m\right] \nonumber \\
		& = (S_m - n\mu) + \sum_{k = m+1}^{n} \mathbb{E}[X_k \mid \mathcal{F}_m] \nonumber \\
		& = (S_m - n\mu) + \sum_{k = m+1}^{n} \mathbb{E}[X_k]  \label{aaa} \\
		& = S_m - m\mu. \nonumber
	\end{align}
	Donde (\ref{aaa}) se cumple pues $\mathcal{F}_m$ es la información generada por las variables $X_0, X_1, \ldots, X_m$ y entonces $(X_k, k \geq m+1)$ es independiente de $\mathcal{F}_m$. Por lo tanto, $(S_n - n\mu, n \geq 0)$ es una \emph{martingala}.
\end{example}

Una propiedad importante de las martingalas es que poseen una esperanza cuyo valor siempre es constante.
\begin{proposition} \label{martin}
	Si $(X_n, n \geq 0)$ es una martingala, entonces $\mathbb{E}[X_n] = \mathbb{E}[X_0]$, para toda $n \geq 0$.
\end{proposition}
\begin{proof}
	Si $(X_n, n \geq 0)$ es una martingala entonces tenemos que $X_0 = \mathbb{E}[X_n \mid \mathcal{F}_0]$ para $n \geq 0$ por lo que
	\begin{align*}
		\mathbb{E}[X_0] & = \mathbb{E}[\mathbb{E}[X_n \mid \mathcal{F}_0]] = \mathbb{E}[X_n],
	\end{align*}
para toda $n \geq 0$.
\end{proof}

Por otro lado, en general no sucede que si un proceso tiene un valor esperado constante, este proceso resulte ser una martingala. Sin embargo, con las condiciones necesarias la afirmación anterior llega a ocurrir. Para poder mostrar el resultado requerimos de una definición que será vital para desarrollo de este capítulo.

\begin{definition}[Tiempo de Paro]
Una variable aleatoria $\tau: \Omega \rightarrow \bar{\mathbb{N}} = \mathbb{N} \cup \{ \infty \}$ es un tiempo de paro con respecto a ($\mathcal{F}_n$) si $\{ \tau \leq n \} \in \mathcal{F}_n$.
\end{definition}

Los tiempos de paro pueden ser pensados como aquel tiempo en el que ocurre algún evento de interés, con la convención de que el tiempo de paro toma el valor $+ \infty$ si el evento nunca ocurre. El término ``tiempo de paro" proviene de un concepto involucrado con los juegos de apuestas: dependiendo de ciertos eventos, un apostador puede dejar el juego en cualquier momento (un tiempo aleatorio), pero en el momento en que decide detenerse, esa decisión debe estar basada en los eventos que han ocurrido en el pasado y no en eventos futuros.

\begin{proposition} 
\label{espconst}
	Sea $\tau$ un tiempo de paro acotado por $c$, es decir, $\mathbb{P}(\tau \leq c) = 1$ y sea $(X_n, n \geq 0)$ una martingala, entonces  $\mathbb{E}[X_{\tau}] = \mathbb{E}[X_0]$.
\end{proposition}
\begin{proof}
	Observemos que a $X_{\tau}$ la podemos escribir como,
	\begin{align*}
		X_{\tau} = \sum_{n=0}^{\infty} X_n1_{ \{\tau = n\} }.
	\end{align*}
	Entonces, sea $[c]$ la parte entera de $c$ o el máximo entero no superior a $c$,
	\begin{align*}
		\mathbb{E}[X_{\tau}] & = \mathbb{E}\left[\sum_{n=0}^{\infty} X_n 1_{ \{\tau = n\} } \right] \\
		& = \mathbb{E}\left[\sum_{n=0}^{[c]} X_n  1_{ \{\tau = n\} }\right] \\
		& = \sum_{n=0}^{[c]} \mathbb{E}[X_n 1_{ \{\tau = n\} }] \\
		& = \sum_{n=0}^{[c]} \mathbb{E}[\mathbb{E}[X_[c] \mid \mathcal{F}_n] 1_{ \{\tau = n\} }].
	\end{align*}
	Notemos que podemos escribir a $\{ \tau = n\}$ como $\{ \tau \leq n\} \setminus \{ \tau \leq n-1\}$ donde ambos eventos pertenecen a $\mathcal{F}_n$ por ser $\tau$ un tiempo de paro, por lo tanto $\{ \tau = n\} \in \mathcal{F}_n$, entonces
	\begin{align*}
		 \mathbb{E}[X_{\tau}] & = \sum_{n=0}^{[c]} \mathbb{E}[\mathbb{E}[X_{[c]} 1_{ \{\tau = n\} } \mid \mathcal{F}_n]] \\
		 & = \sum_{n=0}^{[c]} \mathbb{E}[X_{[c]}  1_{ \{\tau = n\} }] \\
		 & = \mathbb{E}\left[\sum_{n=0}^{[c]} X_c 1_{ \{\tau = n\} }\right] \\
		 & = \mathbb{E}\left[X_{[c]}  \sum_{n=0}^{[c]} 1_{ \{\tau = n\} }\right] \\
		 & = \mathbb{E}[X_{[c]}] = \mathbb{E}[X_0].
	\end{align*}
\end{proof}

La siguiente definición muestra que para un tiempo de paro $\tau$, $\mathcal{F}_{\tau}$ puede ser interpretada la información disponible para algún momento aleatorio. Esta $\sigma$-algebra es conocida como \emph{$\sigma$-algebra parada} y será de utilidad más adelante.

\begin{definition}  \label{algebraaleatoria}
	Sea $\tau$ un tiempo de paro, entonces tenemos que la $\sigma$-algebra parada en $\tau$ esta definida como,
	\begin{align*}
	\mathcal{F}_{\tau} = \{ \Lambda \in \mathcal{F} \mid \Lambda \cap \{ \tau \leq n \} \in \mathcal{F}_n, \text{ para toda } n \}.
	\end{align*}
\end{definition}

Veamos que la definición anterior tiene sentido, al probar que $\mathcal{F}_{\tau}$ es, efectivamente, una $\sigma$-algebra.
\begin{proposition}
	Sea $\tau$ un tiempo de paro, entonces $\mathcal{F}_{\tau}$ es una $\sigma$-algebra.
\end{proposition}
\begin{proof}
	Como $\{ \tau \leq n \} = \Omega \cap \{ \tau \leq n \} \in \mathcal{F}_n$ para toda $n$, de acuerdo a la definición de $\mathcal{F}_{\tau}$ se tiene que $\Omega \in \mathcal{F}_{\tau}$. Si $A \in \mathcal{F}_{\tau}$, podemos escribir a $A^{c} \cap B$ como $B \setminus (A \cap B)$ entonces:
	\begin{align}
		A^{c} \cap \{ \tau \leq n \} = \{ \tau \leq n \} \setminus (A \cap \{ \tau \leq n \}). \label{aab}
	\end{align}
	Donde, a la derecha de (\ref{aab}), ambas partes pertenecen a $\mathcal{F}_{n}$, por lo tanto, $A^{c} \in \mathcal{F}_{\tau}$. Por último, si $(A_k, k \geq 0) \in \mathcal{F}_{\tau}$ entonces observemos que
	\begin{align*}
		\left(\bigcup_{k=0}^{\infty} A_k\right) \cap \{ \tau \leq n \} = \bigcup_{k=0}^{\infty} (A_k \cap \{ \tau \leq n \}).
	\end{align*}
	Se tiene entonces que $\bigcup_{k=0}^{\infty} A_k \in \mathcal{F}_{\tau}$. \\
	
	Como $\mathcal{F}_{\tau}$ contiene al conjunto $\Omega$, y al ser cerrado bajo complementos y uniones contables,  $\mathcal{F}_{\tau}$ es una $\sigma$-algebra.
\end{proof}

Verifiquemos dos sencillas proposiciones que serán de ayuda a la hora de presentar un resultado importante, el  Teorema de Paro Opcional de Doob.
\begin{proposition}
\label{inclu}
	Sean dos tiempos de paro, $\gamma$ y $\tau$, tales que $\gamma \leq \tau$, entonces $\mathcal{F}_{\gamma} \subset \mathcal{F}_{\tau}$.
\end{proposition}
\begin{proof}
	Si sabemos que $\gamma \leq \tau$, entonces $\{ \tau \leq n \} \subset \{ \gamma \leq n \}$, y por lo tanto $\{ \tau \leq n \} = \{ \gamma \leq n \} \cap \{ \tau \leq n \}$. Si $\Lambda \in \mathcal{F}_{\gamma}$ entonces
	\begin{align*}
		\Lambda \cap \{ \tau \leq n \} = \Lambda \cap \{ \gamma \leq n \} \cap \{ \tau \leq n \}.
	\end{align*}
	Donde $\Lambda \cap \{ \gamma \leq n \} \in \mathcal{F}_n$ por la definición de $\mathcal{F}_{\gamma}$, de igual manera $\{ \tau \leq n \} \in \mathcal{F}_n$ por ser $\tau$ un tiempo de paro. Por lo tanto, $\Lambda \cap \{ \tau \leq n \} \in \mathcal{F}_n$ para toda $n$, es decir, $\Lambda \in \mathcal{F}_{\tau}$.
\end{proof}

\begin{proposition}
\label{medible}
	Supongamos que $(X_n, n \geq 0)$ es una sucesión de variables aleatorias tales que para toda $n$ se tiene que $X_n$ es $\mathcal{F}_n$ medible. Consideremos un tiempo de paro $\tau$, entonces $X_{\tau}$ es $\mathcal{F}_{\tau}$ medible.
\end{proposition}
\begin{proof}
	Recordemos que podemos reescribir el término $X_{\tau}$ como
	\begin{align*}
		X_{\tau} = \sum_{n=0}^{\infty} X_n  1_{ \{\tau = n\} }.
	\end{align*}
	Si consideremos un boreliano $B$, es necesario mostrar que el evento $\{ X_{\tau} \in B \} \in \mathcal{F}_{\tau}$, es decir, $\{ X_{\tau} \in B \} \cap \{ \tau \leq n\} \in \mathcal{F}_{n}$ para toda $n$. Por lo tanto,
	\begin{align*}
		\{ X_{\tau} \in B \} \cap \{ \tau \leq n\} & = \bigcup_{i=0}^{n} \{ X_{\tau} \in B \} \cap \{ \tau = i\} \\
		& = \bigcup_{i=0}^{n} \{ X_{i} \in B \} \cap \{ \tau = i\}.
	\end{align*}
	Como $\{ X_{i} \in B \} \cap \{ \tau = i\} \in \mathcal{F}_i \subset \mathcal{F}_n$ para toda $i \leq n$, entonces tenemos que $\{ X_{\tau} \in B \} \cap \{ \tau \leq n\} \in \mathcal{F}_{n}$, por lo que $X_{\tau}$ es $\mathcal{F}_{\tau}$ medible.
\end{proof}

Los siguientes teoremas muestran un poderoso resultado, que liga los conceptos presentados hasta ahora. Veamos que la propiedad de martingala se mantiene aún cuando se consideren tiempos de paro, en lugar de tiempos conocidos.

\begin{theorem}[Teorema de Paro Opcional de Doob] 
\label{opcional}
	Sea $X = (X_n, n \geq 0)$ una martingala, considere dos tiempos de paro $\gamma$ y $\tau$, acotados por una constante $K$, con $\gamma \leq \tau$ c.s., entonces
	\begin{align*}
	\mathbb{E}[X_{\tau} \mid \mathcal{F}_{\gamma}] = X_{\gamma}, \hspace{0.3cm} \text{c.s.}
	\end{align*}
\end{theorem}
\begin{proof}
	Recordemos que la definición de esperanza condicional nos dice que \cite[p.~200]{jacodprotter}: si una variable $Y \in \mathcal{L}^2(\Omega, \mathcal{A}, \mathbb{P})$ y consideramos $\mathcal{G}$ una sub $\sigma$-algebra de $\mathcal{A}$, entonces existe un elemento $\mathbb{E}[Y \mid \mathcal{G}] \in \mathcal{L}^2(\Omega, \mathcal{A}, \mathbb{P})$ donde la siguiente condición se cumple
	\begin{align*}
		\mathbb{E}[YX] = \mathbb{E}[\mathbb{E}[Y \mid \mathcal{G}] X] \hspace{0.3cm} \text{para toda } X \in \mathcal{L}^2(\Omega, \mathcal{A}, \mathbb{P}).
	\end{align*}
	Al ser $\tau$ y $\gamma$ tiempos acotados por una constante $K \in \mathbb{N}$, tenemos que $|X_{\tau}| \leq \sum_{n=0}^{K} |X_n|$, por lo que $X_{\tau}$ es integrable y por la misma razón, $X_{\gamma}$ también es integrable. Por la Proposición \ref{medible} sabemos que $X_{\gamma}$  es $\mathcal{F}_{\gamma}$ medible. \\
	
	Basta probar entonces que $\mathbb{E}[X_{\tau} Z] = \mathbb{E}[X_{\gamma} Z]$ para toda variable aleatoria $\mathcal{F}_{\gamma}$ medible. Resulta equivalente a mostrar que si $\Lambda \in \mathcal{F}_{\gamma}$ entonces,
	\begin{align*}
		\mathbb{E}[X_{\tau} 1_{\Lambda}] = \mathbb{E}[X_{\gamma} 1_{\Lambda}].
	\end{align*}
	Puesto que podemos aproximar una variable aleatoria cualquiera a partir de variables aleatorias simples y usando el Teorema de Convergencia Dominada de Lebesgue el cual afirma que \cite[p.~187]{shiryaev}: si $\lim_{n \rightarrow \infty} X_n = X$ c.s. y $|X_n| \leq K$ c.s., entonces $\lim_{n \rightarrow \infty} \mathbb{E}[X_n \mid \mathcal{F}] = \mathbb{E}[X \mid \mathcal{F}]$. \\
	
\noindent Definamos un nuevo tiempo aleatorio $\phi$ con $\Lambda \in \mathcal{F}_{\gamma}$,
	\begin{align*}
		\phi(\omega) = \gamma(\omega)1_{\Lambda}(\omega) + \tau(\omega)1_{\Lambda^c}(\omega).
	\end{align*}
Veamos que $\{\phi \leq n\} = (\Lambda \cap \{\gamma \leq n\}) \cup (\Lambda^c \cap \{\tau \leq n\})$, entonces, sabemos que como $\Lambda \in \mathcal{F}_{\gamma}$ tenemos que $(\Lambda \cap \{\gamma \leq n\}) \in \mathcal{F}_n$. Por otro lado, como $\Lambda \in \mathcal{F}_{\gamma}$ entonces $\Lambda^c \in \mathcal{F}_{\gamma}$ y por la Proposición \ref{inclu}, tenemos que $\Lambda^c \in \mathcal{F}_{\tau}$ por lo que $(\Lambda^c \cap \{\tau \leq n\}) \in \mathcal{F}_n$. Por lo tanto, $\phi$ es un tiempo de paro.\\
	
Por el hecho anterior y la Proposición \ref{espconst} sabemos que $\mathbb{E}[X_{\phi}] =  \mathbb{E}[X_0] = \mathbb{E}[X_{\tau}]$. Por último, veamos que,
	\begin{align*}
		\mathbb{E}[X_{\phi}] & = \mathbb{E}[X_{\gamma}1_{\Lambda}] + \mathbb{E}[X_{\tau}1_{\Lambda^c}], \\
		\mathbb{E}[X_{\tau}] & = \mathbb{E}[X_{\tau}1_{\Lambda}] + \mathbb{E}[X_{\tau}1_{\Lambda^c}].
	\end{align*}
	Por lo que, al sustraer $\mathbb{E}[X_{\phi}]$ de $\mathbb{E}[X_{\tau}]$ obtenemos
	\begin{align*}
		\mathbb{E}[X_{\tau}] & - \mathbb{E}[X_{\phi}] = 0, \\
		(\mathbb{E}[X_{\tau}1_{\Lambda}] + \mathbb{E}[X_{\tau}1_{\Lambda^c}]) & - (\mathbb{E}[X_{\gamma}1_{\Lambda}] + \mathbb{E}[X_{\tau}1_{\Lambda^c}]) = 0, \\
		\mathbb{E}[X_{\tau}1_{\Lambda}] & - \mathbb{E}[X_{\gamma}1_{\Lambda}] = 0. \\
	\end{align*}
	Por lo tanto, $\mathbb{E}[X_{\tau}1_{\Lambda}] = \mathbb{E}[X_{\gamma}1_{\Lambda}]$ para todo $\Lambda \in \mathcal{F}_{\gamma}$.
\end{proof}

Con las conclusiones vistas hasta ahora podemos establecer un resultado parcial del recíproco de la Proposición \ref{martin}, el cual nos asegura que, dadas las condiciones adecuadas, podemos afirmar que un proceso con esperanza constante, resulta ser una martingala.

\begin{theorem}
\label{condmartin}
	Sea $(X_n, n \geq 0)$ una sucesión de variables aleatorias, tal que $X_n$ es $\mathcal{F}_n$-medible para toda $n$. Supongamos que $\mathbb{E}[|X_n|] < \infty$ para toda $n$ y que $\mathbb{E}[X_{\tau}] = \mathbb{E}[X_0]$ para todo tiempo de paro acotado $\tau$. Se tiene que, $(X_n, n \geq 0)$ es una martingala.
\end{theorem}
\begin{proof}
	Para verificar que $(X_n, n \geq 0)$ es una martingala debemos verificar que se cumpla que para todo tiempo $0 \leq m < n < \infty$
	\begin{align*}
		\mathbb{E}[X_n \mid \mathcal{F}_m] = X_m.
	\end{align*}
	Donde $\mathbb{E}[X_n \mid \mathcal{F}_m]$ representa el valor esperado del proceso al tiempo $n$ condicionado con respecto a la información a tiempo $m$. De la misma manera que en el Teorema \ref{opcional}, basta con verificar que $\mathbb{E}[X_n 1_{\Lambda}] = \mathbb{E}[X_m 1_{\Lambda}]$ para toda $\Lambda \in \mathcal{F}_m$.
	Sea $0 \leq m < n < \infty$ y $\Lambda \in \mathcal{F}_m$. Definamos el tiempo aleatorio $\tau$ como
	\begin{align*}
		\tau (\omega) =
		\begin{cases}
			n & \text{ si } \omega \in \Lambda, \\
			m & \text{ si } \omega \notin \Lambda.
		\end{cases}
	\end{align*}
	Podemos verificar fácilmente que $\tau$ es un tiempo de paro, ya que $\{\tau \leq n\} = (\Lambda \cap \{n = n\}) \cup (\Lambda^c \cap \{m \leq n\})$. Si $\omega \in \Lambda$ entonces $\{\tau = n\} \in \mathcal{F}_n$, además $\Lambda \in \mathcal{F}_m \subset \mathcal{F}_n$. Por otro lado, si $\omega \notin\Lambda$, entonces $\{m \leq n\} \in \mathcal{F}_m \subset \mathcal{F}_n$ y como $\Lambda \in \mathcal{F}_m$ entonces $\Lambda^c \in \mathcal{F}_m \subset \mathcal{F}_n$. \\
	
\noindent Al ser $\tau$ un tiempo de paro tenemos que
	\begin{align*}
		\mathbb{E}[X_0] = \mathbb{E}[X_{\tau}] = \mathbb{E}[X_m1_{\Lambda^c} + X_n1_{\Lambda}].
	\end{align*}
	Por otro lado
	\begin{align*}
		\mathbb{E}[X_0] = \mathbb{E}[X_m1_{\Lambda^c} + X_m1_{\Lambda}].
	\end{align*}
	Sustrayendo $\mathbb{E}[X_0]$ de $\mathbb{E}[X_{\tau}]$ tenemos
	\begin{align*}
		\mathbb{E}[X_m1_{\Lambda^c}] + \mathbb{E}[X_n1_{\Lambda}] & - (\mathbb{E}[X_m1_{\Lambda^c}] + \mathbb{E}[X_m1_{\Lambda}]) = 0, \\
		\mathbb{E}[X_n1_{\Lambda}] & -  \mathbb{E}[X_m1_{\Lambda}] = 0.
	\end{align*}
	Por lo tanto, $\mathbb{E}[X_n1_{\Lambda}] = \mathbb{E}[X_m1_{\Lambda}]$ para toda $\Lambda \in \mathcal{F}_m$, demostrando así que $\mathbb{E}[X_n \mid \mathcal{F}_m] = X_m$, es decir, $(X_n, n \geq 0)$ es una martingala. 
\end{proof}

\section{Supermartingalas y Submartingalas}
Para llegar a un segundo resultado importante en la teoría de martingalas, es necesario introducir un nuevo concepto asociado con nuestra Definición \ref{martingala}. Si reemplazamos la igualdad por una desigualdad en las condiciones dadas de la definición de martingala obtenemos el siguiente concepto. \\

Consideremos un espacio de probabilidad dado $(\Omega, \mathcal{F}, \mathbb{P})$ y una sucesión creciente de $\sigma$-algebras $(\mathcal{F}_n, n \geq 0)$.

\begin{definition}
\label{submartin}
	Una sucesión de variables $(X_n, n \geq 0)$ es llamada \emph{submartingala} si,
	\begin{enumerate}
		\item $\mathbb{E}[|X_n|] < \infty$ para cada $n$;
		\item $X_n$ es $\mathcal{F}_n$ medible, para cada $n$;
		\item $\mathbb{E}[X_n \mid \mathcal{F}_m] \geq X_m$ c.s., para todo $m \leq n$.
	\end{enumerate}
\end{definition}

Si $(X_n, n \geq 0)$ es una submartingala, entonces definimos al proceso $(- X_n, n \geq 0)$, para toda $n \geq 0$, es una \emph{supermartingala}. La sucesión $(X_n, n \geq 0)$ es una martingala si y solo si es una submartingala y al mismo tiempo una supermartingala. \\

Recordemos que la desigualdad de Jensen nos asegura lo siguiente \cite[p.~205]{jacodprotter}: si una función $\phi: \mathbb{R} \rightarrow \mathbb{R}$ es convexa, y las variables aleatorias $X$ y $\phi(X)$ son integrables, entonces para cualquier $\sigma$-algebra $\mathcal{F}$ se tiene que $\phi(\mathbb{E}[X \mid \mathcal{F}]) \leq \mathbb{E}[\phi(X) \mid \mathcal{F}]$. A través de este resultado podemos ligar las Definiciones \ref{martingala} y \ref{submartin} con la siguiente proposición.

\begin{proposition}
\label{convexa}
	Si $(X_n, n \geq 0)$ es una martingala, coni $\phi$ una función convexa y $\phi(X_n)$ es integrable para toda $n$, entonces $(\phi(X_n), n \geq 0)$ es una submartingala.
\end{proposition}
\begin{proof}
	Sea $m \leq n$, tenemos que $\mathbb{E}[X_n \mid \mathcal{F}_m] = X_m$, c.s., entonces al aplicar $\phi(\mathbb{E}[X_n \mid \mathcal{F}_m]) = \phi(X_m)$, c.s. y como $\phi$ es una función convexa podemos concluir de la desigualdad de Jensen
	\begin{align*}
		\mathbb{E}[\phi(X_n) \mid \mathcal{F}_m] \geq \phi(\mathbb{E}[X_n \mid \mathcal{F}_m]) = \phi(X_m).
	\end{align*}
	Por lo tanto, $(\phi(X_n), n \geq 0)$ es una submartingala.
\end{proof}

Consideremos la función $\phi(x) = |x|$, la cual es una función convexa. Por la Proposición \ref{convexa}, tenemos que si $(M_n, n \geq 0)$ es una martingala entonces $X_n = |M_n|$ para toda $n$ es una submartingala. \\

Recordemos que la propiedad de martingala se mantiene aún usando tiempos de paro acotados (Teorema \ref{opcional}), de igual manera, la propiedad de submartingala se mantiene bajo las mismas condiciones. \\

El siguiente teorema muestra la fuerte relación que existe entre las martingalas y submartingalas. El resultado asegura una descomposición única de un proceso submartingala como la suma de un proceso martingala y un proceso predecible.

\begin{theorem}[Descomposición de Doob]
\label{descdoob}
	Sea $(X_n, n \geq 0)$ es una submartingala. Entonces, se tiene la siguiente descomposición del proceso para cada $n \geq 1$
	\begin{align*}
	X_n = X_0 + M_n + A_n, \hspace{1cm} \text{con    } M_0 = A_0 = 0.
	\end{align*}
	donde $(M_n, n \geq 0)$ es una martingala y $(A_n, n \geq 0)$ satisface que $A_{n+1} \geq A_n$, c.s. y $A_{n+1}$ es $\mathcal{F}_n$-medible para toda $n$. Más aún, la descomposición es única.
\end{theorem}
\begin{proof}
	Como $(X_n, n \geq 0)$ es una submartingala se tiene que $\mathbb{E}[X_{k+1} \mid \mathcal{F}_k] \geq X_k$ entonces $\mathbb{E}[X_{k+1} \mid \mathcal{F}_k] - X_k \geq 0$. Sin embargo, $X_k$ es una variable aleatoria $\mathcal{F}_k$-medible, por lo tanto $\mathbb{E}[X_{k+1} - X_k \mid \mathcal{F}_k] \geq 0$.
	Definamos al proceso $A_0 = 0$ y
	\begin{align*}
	A_n = \sum_{k=1}^{n} \mathbb{E}[X_k - X_{k-1} \mid \mathcal{F}_{k-1}], \hspace{0.3cm} \text{con } n \geq 1.
	\end{align*}
	Por lo tanto, tenemos que $A_n \leq A_{n+1}$ c.s. y además $A_k$ es $\mathcal{F}_{k-1}$-medible. Mostremos la existencia de la martingala $(M_n, n \geq 0)$. \\
	
	Notemos que $X_{n-1}$ es una variable $\mathcal{F}_{n-1}$-medible, entonces $\mathbb{E}[X_n \mid \mathcal{F}_{n-1}] - X_{n-1} = \mathbb{E}[X_n - X_{n-1} \mid \mathcal{F}_{n-1}]$. Por otro lado, 
	\begin{align*}
	A_n - A_{n-1} & = \sum_{k=1}^{n} \mathbb{E}[X_k - X_{k-1} \mid \mathcal{F}_{k-1}] - \sum_{k=1}^{n-1} \mathbb{E}[X_k - X_{k-1} \mid \mathcal{F}_{k-1}] \\
	& = \mathbb{E}[X_n - X_{n-1} \mid \mathcal{F}_{n-1}]. 
	\end{align*}
	
	\noindent Teniendo en cuenta ambas igualdades obtenemos la siguiente igualdad
	\begin{align*}
		\mathbb{E}[X_n \mid \mathcal{F}_{n-1}] - X_{n-1} = A_n - A_{n-1}.
	\end{align*}
	Es decir
	\begin{align*}
		\mathbb{E}[X_n \mid \mathcal{F}_{n-1}] - A_n = X_{n-1} - A_{n-1}.
	\end{align*}
	Sin embargo, por definición sabemos que $A_n \in \mathcal{F}_{n-1}$, entonces 
	\begin{align*}
		\mathbb{E}[X_n - A_n \mid \mathcal{F}_{n-1}] = X_{n-1} - A_{n-1}.
	\end{align*}
	Por tanto, si definimos a $M_n = X_n - A_n$ para toda $n$, tenemos que $(M_n, n \geq 0)$ es una martingala. Aseguramos entonces la existencia de la descomposición. \\
	
	Para demostrar la unicidad de la descomposición, supongamos que para la submartingala $(X_n, n \geq 0)$ existen dos descomposiciones tales que
	\begin{align*}
		X_n &= X_0 + M_n + A_n, \hspace{0.3cm} n \geq 1, \\
		X_n &= X_0 + L_n + B_n, \hspace{0.3cm} n \geq 1.
	\end{align*}
	De la diferencia entre ambas descomposiciones obtenemos
	\begin{align*}
		M_n + A_n = L_n + B_n, \hspace{0.3cm} n \geq 1.
	\end{align*}
	Es decir, 
	\begin{align*}
		M_n - L_n = B_n - A_n, \hspace{0.3cm} n \geq 1.
	\end{align*}
	Recordemos que por su construcción, $B_n$ y $A_n$ son variables $\mathcal{F}_{n-1}$ medibles, entonces $B_n - A_n \in \mathcal{F}_{n-1}$, por lo tanto, $M_n - L_n \in \mathcal{F}_{n-1}$. Tenemos entonces que $M_n - L_n = \mathbb{E}[M_n - L_n \mid \mathcal{F}_{n-1}]$. Por otro lado, sabemos que $M_n$ y $L_n$ son martingalas, entonces
	\begin{align*}
		M_n - L_n = \mathbb{E}[M_n - L_n \mid \mathcal{F}_{n-1}] = M_{n-1} - L_{n-1} = B_{n-1} - A_{n-1}, \hspace{0.3cm} \text{c.s.}
	\end{align*}
	Siguiendo inductivamente este procedimiento llegamos a que
	\begin{align*}
		M_n - L_n = M_0 - L_0, \hspace{0.3cm} \text{c.s.}
	\end{align*}
	Por hipótesis sabemos que $M_0 = L_0 = 0$, y entonces podemos asegurar la unicidad de la descomposición ya que $M_n = L_n$, c.s. y ademas $B_n = A_n$, c.s.
\end{proof}

\begin{corollary}
	Sea $(X_n, n \geq 0)$ es una supermartingala. Entonces, existe una única descomposición del proceso para cada $n \geq 1$,
	\begin{align*}
		X_n = X_0 + M_n - A_n,
	\end{align*}
	con $M_0 = A_0 = 0$, $(M_n, n \geq 0)$ una martingala y un proceso $(A_n, n \geq 0)$ tal que $A_{n+1} \geq A_n$ c.s. y $A_{n+1}$ es $\mathcal{F}_n$-medible para toda $n$.
\end{corollary}
\begin{proof}
	Si definimos a $Y_n = - X_n$ entonces, el proceso $(Y_n, n \geq 0)$ es una submartingala. Por el Teorema \ref{descdoob}, tenemos la siguiente descomposición para el proceso
	\begin{align*}
		Y_n = Y_0 + L_n + B_n,
	\end{align*}
Entonces $X_n = X_0 - L_n - B_n$, al definir $M_n = - L_n$ y $A_n = B_n$ para toda $n \geq 1$.
\end{proof}

\section{Desigualdades Maximales}
A continuación mostramos las principales desigualdades para martingalas, las cuales serán una herramienta para el estudio de la convergencia de martingalas. Consideremos un espacio de probabilidad fijo y conocido $(\Omega, \mathcal{F}, \mathbb{P})$ y una sucesión de $\sigma$-algebras $(\mathcal{F}_n)_{n \geq 0}$ tales que $\mathcal{F}_n \subset \mathcal{F}_{n+1} \subset \mathcal{F}$ para toda $n$. Consideremos una sucesión de variables aleatorias integrables $(M_n, n \geq 0)$ y además, para cada $n$, se tiene que $M_n$ es $\mathcal{F}_n$-medible. Definamos
\begin{align*}
	M_n^{*} = \sup_{j \leq n} |M_j|.
\end{align*}
Es claro que $M_n^{*} \leq M_{n+1}^{*}$, entonces para toda $m \leq n$ se tiene que $M_m^{*} = \mathbb{E}[M_m^{*} \mid \mathcal{F}_m] \leq \mathbb{E}[M_{n}^{*} \mid \mathcal{F}_m]$ c.s., además 
\begin{align*}
	M_n^{*} = \sup_{j \leq n} |M_j| \leq \sum_{j=0}^n |M_j|.
\end{align*}
Por lo tanto, $\mathbb{E}[M_n^{*} ]\leq \mathbb{E}[\sum_{j=0}^n |M_j|] < \infty$. Con estas condiciones, aseguramos que el proceso $(M_n^{*}, n \geq 0)$ es una submartingala. Estamos interesados en conocer la probabilidad de que el valor $M_n^{*}$ supere una cantidad $\alpha$. \\

Recordemos que la \emph{Desigualdad de Markov} afirma que \cite[p.~29]{jacodprotter} para una variable aleatoria $X: \Omega \rightarrow \mathbb{R}$ se tiene que $\mathbb{P}(|X| \geq \alpha) \leq \mathbb{E}[|X|]/\alpha$ para toda $\alpha > 0$, entonces
\begin{align}
	\mathbb{P}(M_n^{*} \geq \alpha) \leq \frac{\mathbb{E}[M_n^{*}]}{\alpha}. \label{aac}
\end{align}
En el caso donde $(M_n, n \geq 0)$ no solo cumple con las condiciones mencionadas, sino que además satisface que es una martingala, podemos reemplazar $M_n^{*}$ por $|M_n|$ en el lado derecho de (\ref{aac}), dando como resultado el siguiente teorema.

\begin{theorem}[Primera Desigualdad de Martingalas de Doob] 
\label{primera}
	Sea $(M_n, n \geq 0)$ una martingala o una submartingala positiva. Entonces
	\begin{align*}
		\mathbb{P}(M_n^{*} \geq \alpha) \leq \frac{\mathbb{E}[|M_n|]}{\alpha}.
	\end{align*}
\end{theorem}
\begin{proof}
	Consideremos en primer lugar el siguiente tiempo de paro
	\begin{align*}
		\tau = \min \{j : |M_j| \geq \alpha\}.
	\end{align*}
	Veamos que sin importar que caso consideremos para $(M_n, n \geq 0)$ (martingala o submartingala positiva), $(|M_n|, n \geq 0)$ es una submartingala. \\ 
	
	Si $M$ una martingala, por la Proposición \ref{convexa} tenemos que $(|M_n|, n \geq 0)$ es una submartingala. Por otro lado, si $M$ es una submartingala positiva entonces resulta que $|M_n| = M_n$ para toda $n$. Por lo que $(|M_n|, n \geq 0)$ es una submartingala.
	% $M_m \leq \mathbb{E}[M_n \mid \mathcal{F}_m]$ c.s. para toda $m \leq n$, además, como $M$ es positiva para toda $n$ tenemos que $M_n > 0$, entonces, $|M_n|$ es creciente para toda $n$, ya que $\phi(x) = |x|$ es una función convexa y creciente en $\mathbb{R}_{+}$, por lo tanto
	\begin{align}
	|M_m| \leq  |\mathbb{E}[M_n \mid \mathcal{F}_m]|. \label{corrJC1}
	\end{align}
De la desigualdad de Jensen tenemos que 
	\begin{align}
	|\mathbb{E}[M_n \mid \mathcal{F}_m]| \leq \mathbb{E}[|M_n| \mid \mathcal{F}_m]. \label{corrJC2}
	\end{align}
De (\ref{corrJC1}) y (\ref{corrJC2}) tenemos que $(|M_n|, n \geq 0)$ es una submartingala. \\
	
	Observemos que el evento $\{ \tau \leq n, |M_{\tau}| \geq \alpha\}$ representa el primer momento en el que ocurre la condición $\{|M_{\tau}| \geq \alpha\}$ que resulta ser el mismo evento que $\{\sup_{j \leq n} |M_j| = M_n^{*} \geq \alpha\}$, entonces
	\begin{align}
		\mathbb{P}(M_n^{*} \geq \alpha) = \mathbb{P}(\tau \leq n, |M_{\tau}| \geq \alpha). \label{aad}
	\end{align}
Utilizando la desigualdad de Markov tenemos que 
	\begin{align}
		\mathbb{P}(\tau \leq n, |M_{\tau}| \geq \alpha) = \mathbb{P}(M_n^{*} \geq \alpha)  \leq \frac{\mathbb{E}[M_n^{*}]}{\alpha} = \mathbb{E} \left[\frac{|M_{\tau}|}{\alpha}  1_{\{\tau \leq n\}} \right]. \label{aae}
	\end{align}
Veamos por otro lado, en el conjunto $\{ \tau \leq n\}$ tenemos que $M_{\tau} = M_{\tau \wedge n}$. De (\ref{aad}) y (\ref{aae}) tenemos que
	\begin{align*}
		\mathbb{P}(M_n^{*} \geq \alpha) \leq \frac{1}{\alpha} \mathbb{E}[|M_{\tau}|  1_{\{\tau \leq n\}}] = \frac{1}{\alpha} \mathbb{E}[|M_{\tau \wedge n}|  1_{\{\tau \leq n\}}].
	\end{align*}
Además tenemos que 
	\begin{align*}
		\frac{1}{\alpha} \mathbb{E}[|M_{\tau \wedge n}|  1_{\{\tau \leq n\}}] \leq \frac{1}{\alpha} \mathbb{E}[|M_{\tau \wedge n}|].
	\end{align*}
Por lo tanto
	\begin{align}
		\mathbb{P}(M_n^{*} \geq \alpha) \leq \frac{1}{\alpha} \mathbb{E}[|M_{\tau}|  1_{\{\tau \leq n\}}] \leq \frac{\mathbb{E}[|M_{\tau \wedge n}|]}{\alpha}. \label{aaf}
	\end{align}
	Por último, con una prueba análoga de la Proposición \ref{espconst} se sabe que para un tiempo de paro $\tau$ acotado por $c \in \mathbb{N}$ y una submartingala $(X_n, n \geq 0)$ se tiene que $\mathbb{E}[X_{\tau}] \leq \mathbb{E}[X_{c}]$. Entonces, 
	%recordemos que de la Proposición \ref{espconst} y las observaciones de la Proposición \ref{convexa} tenemos que la propiedad de submartingala se mantiene aún usando tiempos aleatorios, y por lo tanto obtenemos la siguiente desigualdad para el tiempo de paro $(\tau \wedge n)$
	\begin{align}
		\frac{\mathbb{E}[|M_{\tau \wedge n}|]}{\alpha} \leq \frac{\mathbb{E}[|M_{n}|]}{\alpha} \label{aag}
	\end{align}
	De (\ref{aaf}) y (\ref{aag}) tenemos la desigualdad deseada
	\begin{align*}
		\mathbb{P}(M_n^{*} \geq \alpha) \leq \frac{\mathbb{E}[|M_{n}|]}{\alpha}.
	\end{align*}
\end{proof}

Será de mucha ayuda mostrar un sencillo pero valioso resultado que nos permitirá verificar, más adelante, nuestra segunda desigualdad de martingalas.

\begin{lemma}
\label{repint}
Sean $p \geq 1$ y $X \geq 0$ una variable aleatoria, tal que $\mathbb{E}[X^p] < \infty$. Entonces
	\begin{align*}
		\mathbb{E}[X^p] = \int_0^{\infty} p \lambda^{p-1} \mathbb{P}(X > \lambda) d\lambda.
	\end{align*}
\end{lemma}
\begin{proof}
De las propiedades de la esperanza tenemos que
	\begin{align*}
		\int_0^{\infty} p \lambda^{p-1} \mathbb{P}(X > \lambda) d\lambda = \int_0^{\infty} p \lambda^{p-1} \mathbb{E}[1_{\{X > \lambda\}}] d\lambda = \int_0^{\infty} p \lambda^{p-1} \int_{\Omega} 1_{\{X > \lambda\}} d\mathbb{P} d\lambda.
	\end{align*}
Del teorema de Fubini,
	\begin{align*}
		\int_0^{\infty} p \lambda^{p-1} \int_{\Omega} 1_{\{X > \lambda\}} d\mathbb{P} d\lambda & = \int_0^{\infty} \int_{\Omega} p \lambda^{p-1} 1_{\{X > \lambda\}} d\mathbb{P} d\lambda \\
		& = \int_{\Omega} \int_0^{\infty} p \lambda^{p-1} 1_{\{X > \lambda\}} d\lambda d\mathbb{P} \\
		& = \int_{\Omega} \int_0^{X} p \lambda^{p-1} d\lambda d\mathbb{P} \\
		& = \mathbb{E}\left[ \int_0^{X} p \lambda^{p-1} d\lambda \right].
	\end{align*}
	Por lo tanto, 
	\begin{align*}
		\int_0^{\infty} p \lambda^{p-1} \mathbb{P}(X > \lambda) d\lambda = \mathbb{E}[X^p].
	\end{align*}
\end{proof}

\begin{theorem}[Desigualdad de Doob en $L^p$ para Martingalas]
\label{lp}
	Sea $M = (M_n, n \geq 0)$ una martingala o una submartingala positiva. Sea $1 < p < \infty$, entonces existe una constante $c$ que depende solamente de $p$ tal que
	\begin{align*}
		\mathbb{E}[(M_n^{*})^p] \leq c\mathbb{E}[|M_n|^p].
	\end{align*}
\end{theorem}
\begin{proof}
	Consideremos el caso en que $M$ es una martingala. Como $\phi(x) = |x|$ es una función convexa entonces por la Proposición \ref{convexa} tenemos que $|M|$ es una submartingala. Consideremos además los siguiente procesos para una $n$ fija,
	\begin{align*}
		X_n & = M_n1_{\{|M_n| > \frac{\alpha}{2} \} }, \\
		Z_j & = \mathbb{E}[X_n \mid \mathcal{F}_j], \hspace{0.3cm} 0 \leq j \leq n.
	\end{align*}
	Notemos que para un valor $n$ conocido se tiene que $\mathbb{E}[Z_m \mid \mathcal{F}_k] = \mathbb{E}[\mathbb{E}[X_n \mid \mathcal{F}_m] \mid \mathcal{F}_k] = \mathbb{E}[X_n \mid \mathcal{F}_k] = Z_k$ para $k \leq m \leq n$, por lo tanto $Z_j$, $0 \leq j \leq n$ es una martingala. Además
	\begin{align}
		|M_j| & = |\mathbb{E}[M_n \mid \mathcal{F}_j]| \nonumber \\
		& = \left|\mathbb{E}\left[ M_n 1_{\{|M_n > \frac{\alpha}{2}|\}} + M_n 1_{\{|M_n \leq \frac{\alpha}{2}|\}} \mid \mathcal{F}_j \right] \right| \nonumber \\
		& = \left| \mathbb{E}\left[X_n + M_n 1_{\{|M_n \leq \frac{\alpha}{2}|\}} \mid \mathcal{F}_j \right] \right| \nonumber \\
		& \leq \left| \mathbb{E}\left[ X_n \mid \mathcal{F}_j \right] \right| + \frac{\alpha}{2} \nonumber \\
		& = |Z_j| + \frac{\alpha}{2}. \label{aah}
	\end{align}
	Entonces de (\ref{aah}) tenemos, 
	\begin{align*}
		M_n^{*} = \sup_{j \leq n} |M_j| \leq \sup_{j \leq n} |Z_j| + \frac{\alpha}{2} = Z_n^{*} + \frac{\alpha}{2}.
	\end{align*}
	Por lo tanto, 
	\begin{align}
		\mathbb{P}(M_n^{*} > \alpha) \leq \mathbb{P}(Z_n^{*} + \frac{\alpha}{2} > \alpha) =  \mathbb{P}(Z_n^{*} > \frac{\alpha}{2}). \label{aai}
	\end{align}
	Aplicando la Primera Desigualdad de Doob (Teorema \ref{primera}) a (\ref{aai}) 
	\begin{align*}
		\mathbb{P}(M_n^{*} > \alpha) & \leq \mathbb{P}(Z_n^{*} > \frac{\alpha}{2}) \\
		& \leq \frac{2}{\alpha} \mathbb{E}[|Z_n|] \leq \frac{2}{\alpha} \mathbb{E}[|X_n|] \ (\text{Jensen}) \\
		& = \frac{2}{\alpha} \mathbb{E}[|M_n|1_{\{|M_n| > \frac{\alpha}{2}\}}].
	\end{align*}
	Con este último resultado podemos aplicar a la igualdad del Lema \ref{repint} y verificar la desigualdad deseada
	\begin{align*}
		\mathbb{E}[(M_n^{*})^p] & = \int_0^{\infty} p \lambda^{p-1} \mathbb{P}(M_n^{*} > \lambda) d\lambda \\
		& \leq \int_0^{\infty} p \lambda^{p-1}  \frac{2}{\lambda} \mathbb{E}[|M_n|1_{\{|M_n| > \frac{\lambda}{2}\}}] d\lambda.
	\end{align*}
	Usando el Teorema de Fubini y realizando los cálculos de la integral obtenemos
	\begin{align*}
		& = \int_0^{\infty} 2p \lambda^{p-2} \mathbb{E}[|M_n|1_{\{2|M_n| > \lambda\}} ] d\lambda \\
		& = \mathbb{E}\left[ |M_n| \int_0^{2|Mn|} 2p\lambda^{p-2} d\lambda \right] \\
		& = \frac{2^p p}{p-1} \mathbb{E}[|M_n|^p].
	\end{align*}
	Tenemos entonces que
	\begin{align*}
	\mathbb{E}[(M_n^{*})^p] \leq c\mathbb{E}[|M_n|^p], \hspace{0.3cm} \text{con} \hspace{0.3cm} c \leq \frac{2^p p}{p-1}.
	\end{align*} 
	
Para el caso en que $M$ es una submartingala positiva se procede con la misma demostración, con el conocimiento de que $|M|$ es una submartingala (Teorema \ref{primera}).
\end{proof}

Con respecto al resultado anterior, hemos mostrado la relación existente entre la constante $c$ y el valor $p$, sin embargo, se puede verificar que $c^{\frac{1}{p}} = \frac{p}{p-1}$ y entonces podemos escribir la desigualdad en términos de normas $L^p$.

\begin{theorem}[Desigualdad de Doob en $L^p$ para Martingalas]
\label{lp2}
	Sea $M = (M_n, n \geq 0)$ una martingala o una submartingala positiva. Sea $1 < p < \infty$. Entonces
	\begin{align*}
		||M_n^{*}||_p \leq q ||M_n||_p.
	\end{align*}
donde $||M_n||_p = (\mathbb{E}[|M_n|^p])^{\frac{1}{p}}$ y $q = \frac{p}{p-1} $.
\end{theorem}

Esta desigualdad nos será de gran ayuda a la hora de demostrar uno de los resultados más importantes en teoría de martingalas, el Teorema de Convergencia en Martingalas. Pero antes de demostrar el Teorema \ref{lp2} verifiquemos los siguientes resultados.

\begin{lemma}
\label{lemdes2}
Sea $X = (X_n, n \geq 0)$ una submartingala y $Y = (Y_n, n \geq 0)$ una supermartingala. Entonces para todo $\lambda > 0$
	\begin{align}
		\lambda  \mathbb{P}(X_n^{*} > \lambda) & \leq \mathbb{E}[X_n^{+} 1_{\{X_n^{*} > \lambda\}}] \label{aaj}, \\
		\lambda  \mathbb{P}(Y_n^{*} > \lambda) & \leq \mathbb{E}[Y_0] - \mathbb{E}[Y_n 1_{\{Y_n^{*} \leq \lambda\}}]. \label{aak}
	\end{align}
\end{lemma}
\begin{proof}
Si definimos a $\tau$ de la siguiente manera
	\begin{align*}
	\tau = \inf \{k \leq n \mid X_k \geq \lambda\}
	\end{align*}
Con $\tau = n$, si $X_n^{*} > \lambda$
Entonces tenemos que, como $X$ es una martingala
	\begin{align*}
	0 \leq \mathbb{E}[X_0^{+}] & \leq \mathbb{E}[X_{\tau}^{+}] \\
	& = \mathbb{E}[X_{\tau}^{+} 1_{\{X_n^{*} \leq \lambda\}}] + \mathbb{E}[X_{\tau}^{+} 1_{\{X_n^{*} > \lambda\}}] \\
	& \leq \mathbb{E}[\lambda  1_{\{X_n^{*} \leq \lambda\}}] + \mathbb{E}[X_{\tau}^{+} 1_{\{X_n^{*} > \lambda\}}] \\
	& = \lambda  \mathbb{P}(X_n^{*} \leq \lambda) + \mathbb{E}[X_n^{+} 1_{\{X_n^{*} > \lambda\}}] \\
	& = \lambda  (1 - \mathbb{P}(X_n^{*} > \lambda)) + \mathbb{E}[X_n^{+} 1_{\{X_n^{*} > \lambda\}}] \\
	& \leq - \lambda \mathbb{P}(X_n^{*} > \lambda) + \mathbb{E}[X_n^{+} 1_{\{X_n^{*} > \lambda\}}].
	\end{align*}

Por lo tanto $\lambda  \mathbb{P}(X_n^{*} > \lambda) \leq \mathbb{E}[X_n^{+} 1_{\{X_n^{*} > \lambda\}}]$. \\

Consideremos ahora el caso donde $Y$ es una supermartingala. Definamos a $\tau$ como $\inf \{k \leq n \mid X_k \geq \lambda\}$, donde $\tau = n$ si $X_n^{*} < \lambda$
	\begin{align*}
	\mathbb{E}[Y_0] & \geq \mathbb{E}[Y_{\tau}] \\
	& = \mathbb{E}[Y_{\tau} 1_{\{Y_n^{*} > \lambda\}}] + \mathbb{E}[Y_{\tau} 1_{\{Y_n^{*} \leq \lambda\}}] \\
	& \geq \mathbb{E}[\lambda  1_{\{Y_n^{*} > \lambda\}}] + \mathbb{E}[Y_{\tau} 1_{\{Y_n^{*} \leq \lambda\}}] \\
	& = \lambda  \mathbb{P}(Y_n^{*} > \lambda) + \mathbb{E}[Y_n 1_{\{Y_n^{*} \leq \lambda\}}].
	\end{align*}
\end{proof}
Con el lema anterior podemos verificar el Teorema \ref{lp2}

\begin{proof}
Verifiquemos el caso para una submartingala positiva. Supongamos en primer lugar que
	\begin{align}
	||M_n^{*}||_p = \mathbb{E}[|M_n^{*}|^{\frac{1}{p}}]^p < \infty. \label{aal}
	\end{align}
Del Lema \ref{repint} tenemos que para toda $p > 1$
	\begin{align*}
	\mathbb{E}[M_n^{*}]^p & = \int_0^{\infty} p \lambda^{p-1} \mathbb{P}(M_n^{n} > \lambda) d\lambda.
	\end{align*}
Del Lema \ref{lemdes2} y como $M$ es una submartingala positiva entonces
	\begin{align*}
	\mathbb{E}[M_n^{*}]^p = \int_0^{\infty} p \lambda^{p-1} \mathbb{P}(M_n^{n} > \lambda) d\lambda  \leq \int_0^{\infty} p \lambda^{p-1} \left( \frac{1}{\lambda} \mathbb{E}[M_n 1_{\{M_n^{*} > \lambda\}}] \right) d\lambda.
	\end{align*}
Utilizando el teorema de Fubini obtenemos las siguientes igualdades
	\begin{align}
	\mathbb{E}[M_n^{*}]^p & \leq \int_0^{\infty} p \lambda^{p-1} \left( \frac{1}{\lambda} \mathbb{E}[M_n 1_{\{M_n^{*} > \lambda\}} ] \right) d\lambda \nonumber \\
	& = p  \int_0^{\infty} \lambda^{p-2} \mathbb{E}[M_n 1_{\{M_n^{*} > \lambda\}}] d\lambda \nonumber \\
	& = p \ \mathbb{E} \left[ M_n \int_0^{M_n^{*}} \lambda^{p-2} d\lambda \right] \nonumber \\
	& = \frac{p}{p-1} \mathbb{E}[M_n (M_n^{*})^{p-1}]. \label{aam}
	\end{align}
De la desigualdad de Hölder tenemos que
	\begin{align*}
	\mathbb{E}[M_n^{*}]^p & \leq \frac{p}{p-1} \mathbb{E}[M_n (M_n^{*})^{p-1}] \\
	& \leq q ||M_n||_p ||(M_n^{*})^{p-1}||_q \\
	& = q ||M_n||_p \mathbb{E}[(M_n^{*})^p]^{\frac{1}{q}}.
	\end{align*}
Por lo tanto
	\begin{align*}
	\frac{\mathbb{E}[M_n^{*}]^p}{\mathbb{E}[(M_n^{*})^p]^{\frac{1}{q}}} & \leq q ||M_n||_p, \\
	||M_n^{*}||_p & \leq q ||M_n||_p.
	\end{align*}
Por otro lado, si (\ref{aal}) no se cumple, entonces en (\ref{aam}) consideramos $(M_n^{*} \wedge C)$ en lugar de $M_n^{*}$ con $C$ como una constante, entonces
	\begin{align*}
		\mathbb{E}[M_n^{*} \wedge C]^p \leq q\mathbb{E}[M_n (M_n^{*} \wedge C)^{p-1}] \leq q ||M_n||_p \mathbb{E}[(M_n^{*} \wedge C)^p]^{\frac{1}{q}}.
	\end{align*}
Entonces, del hecho de que $\mathbb{E}[M_n^{*} \wedge C]^p \leq C^p < \infty$, tenemos
	\begin{align*}
		\mathbb{E}[M_n^{*} \wedge c]^p \leq q^p ||M_n||_p^n.
	\end{align*}
Por lo tanto, 
	\begin{align*}
		\mathbb{E}[M_n^{*}]^p = \lim_{C \rightarrow \infty} \mathbb{E}[M_n^{*} \wedge C]^p \leq q^p\mathbb{E}[M_n]^p
	\end{align*}
Por último, el caso en que $M$ es una martingala se reduce a la demostración anterior ya que $|M|^p$ con $p \geq 1$ es una submartingala positiva por la Proposición \ref{convexa}.
\end{proof}

Ahora introducimos el concepto de \emph{cruces ascendentes}, donde utilizaremos la notación de Doob. Sea $(X_n, n \geq 0)$ una submartingala, y sea $a < b$. El número de cruces ascendentes de un intervalo $[a, b]$ es el número de veces en que el proceso comienza por debajo del valor de $a$ y después de algunos pasos salta a algún valor por encima de $b$. Está noción puede ser expresada más adecuadamente usando tiempos de paro. Definamos
\begin{align*}
	\tau_0 = 0,
\end{align*}
inductivamente para $j \geq 0$:
	\begin{align*}
		\eta_{1} = \min \{k > \tau_0 : X_k \leq a \}, & \hspace{1cm} \tau_{1} = \min \{k > \eta_{1} : b \leq X_k \} \\
		\eta_{2} = \min \{k > \tau_1 : X_k \leq a \}, & \hspace{1cm} \tau_{2} = \min \{k > \eta_{2} : b \leq X_k \} \\
		& \vdots \\
		\eta_{j+1} = \min \{k > \tau_j : X_k \leq a \}, & \hspace{1cm} \tau_{j+1} = \min \{k > \eta_{j+1} : b \leq X_k \}
	\end{align*}
Bajo las siguientes condiciones de que el mínimo de un conjunto vacío es $+ \infty$ y el máximo es $0$, entonces podemos definir
	\begin{align}
		U_n^{[a, b]} = \max \{j : \tau_j \leq n\}, \label{aan}
	\end{align}
y $U_n^{[a, b]}$ es el número de cruces ascendentes de $[a, b]$ antes del tiempo $n$.

\begin{theorem}[Desigualdad de Cruces Ascendentes de Doob]
\label{cruces}
Sea $(X_n, n \geq 0)$ una submartingala, sea $a < b$ y sea $U_n^{[a, b]}$ el número de cruces ascendentes de $[a, b]$ antes del tiempo $n$ definida en (\ref{aan}). Entonces
	\begin{align*}
		\mathbb{E}[U_n^{[a, b]}] \leq \frac{1}{b-a} \mathbb{E}[(X_n - a)^{+}].
	\end{align*}
donde $(X_n - a)^{+} = \max (X_n - a, 0)$
\end{theorem}
\begin{proof}
Consideremos al proceso $Y_n = (X_n - a)^{+}$. Por su construcción, $\eta_{n+1} > n$, notemos que podemos descomponer a $Y_n$ como
	\begin{align}
	Y_n = Y_{\eta_1 \wedge n} + \sum_{i = 1}^n (Y_{\tau_i \wedge n} - Y_{\eta_i \wedge n}) + \sum_{i = 1}^n (Y_{\eta_{i+1} \wedge n} - Y_{\tau_i \wedge n}). \label{aao}
	\end{align}
Recordemos que cada cruce ascendente de $X_n$ entre los tiempos $0$ y $n$ corresponde a un entero $i$ tal que $\eta_i < \tau_i \leq n$. Por definición de $Y_n$ tenemos que $Y_{\eta_i} = 0$ y además $Y_{\tau_i \wedge n} = Y_{\tau_i} \geq b-a$. \\

Además por definición de los tiempos de paro $\eta_i$ y $\tau_i$ tenemos que para toda $i$, $Y_{\tau_i \wedge n} - Y_{\eta_i \wedge n} \geq 0$, entonces
	\begin{align}
	(b - a) U_n^{[a, b]} \leq \sum_{i = 1}^n (Y_{\tau_i \wedge n} - Y_{\eta_i \wedge n}). \label{aap}
	\end{align}
Realizando el correspondiente despeje en (\ref{aao}) y sustituyendo en (\ref{aap}) obtenemos
	\begin{align*}
	(b - a) U_n^{[a, b]} \leq Y_n - Y_{\eta_1 \wedge n} - \sum_{i = 1}^n (Y_{\eta_{i+1} \wedge n} - Y_{\tau_i \wedge n}).
	\end{align*}
Y como $Y_{\eta_1 \wedge n} \geq 0$, entonces
	\begin{align}
	(b - a) U_n^{[a, b]} \leq Y_n - \sum_{i = 1}^n (Y_{\eta_{i+1} \wedge n} - Y_{\tau_i \wedge n}). \label{aaq}
	\end{align}
Tomando la esperanza en ambos lados de (\ref{aaq}) y con el hecho de que $\eta_{i+1}$ y $\tau_i$ están acotadas por $n$ entonces y $(Y_n, n \geq 0)$ es una submartingala ya que $\phi(x) = (x-a)^{+}$ es una función convexa entonces, y además $Y_{\eta_{i+1}} \geq Y_{\tau_i}$, es decir, $\mathbb{E} \left[ \sum_{i = 1}^n (Y_{\eta_{i+1} \wedge n} - Y_{\tau_i \wedge n}) \right] \geq 0$
	\begin{align*}
	(b - a) \mathbb{E}[U_n^{[a, b]}] \leq \mathbb{E}[Y_n] - \mathbb{E} \left[ \sum_{i = 1}^n (Y_{\eta_{i+1} \wedge n} - Y_{\tau_i \wedge n}) \right] \leq \mathbb{E}[Y_n].
	\end{align*}
\end{proof}

\section{Teoremas de Convergencia de Martingalas}
En esta última sección presentamos resultados respecto a la convergencia de martingalas, así como el concepto de \emph{uniformemente integrables} para una colección de variables aleatorias, el cual tiene una relación fuerte con la convergencia de martingalas. Concluimos el primer capítulo con la prueba del Teorema del Límite Central de Martingalas análogo al conocido Teorema del Límite Central, pero mostrando las bondades de trabajar con las propiedades de las martingalas.

\begin{theorem}[Teorema de Convergencia de Martingalas]
\label{conver1}
Sea $(X_n, n \geq 0)$ una submartingala tal que $\sup_n \mathbb{E}[X_n^{+}] < \infty$. Entonces $\lim_{n \rightarrow \infty} X_n = X$ existe c.s. (y es finita c.s.). Más aún, $X \in L^1$.
\end{theorem}
\begin{proof}
Sea $U_n^{[a, b]}$ es número de cruces ascendentes de $[a, b]$ antes del tiempo $n$, como se definió en (\ref{aan}). Por su definición $U_n^{[a, b]}$ es no decreciente y acotado, por lo tanto $U(a, b) = \lim_{n \rightarrow \infty} U_n^{[a, b]}$ existe. Por el Teorema de Convergencia Monótona
	\begin{align*}
	\mathbb{E}[U(a, b)] = \mathbb{E}\left[\lim_{n \rightarrow \infty} U_n^{[a, b]}\right] = \lim_{n \rightarrow \infty} \mathbb{E}[U_n^{[a, b]}].
	\end{align*}
Por el Teorema \ref{cruces} tenemos que 
	\begin{align*}
	\mathbb{E}[U(a, b)] & = \lim_{n \rightarrow \infty} \mathbb{E}[U_n^{[a, b]}] \\
	& \leq \frac{1}{b - a} \sup_n \mathbb{E}[(X_n - a)^{+}].
	\end{align*}
Sabemos además, que para todo real $a, x$ se tiene $(x-a)^{+} \leq x^{+} + |a|$ entonces
	\begin{align*}
	\mathbb{E}[U(a, b)] & = \lim_{n \rightarrow \infty} \mathbb{E}[U_n^{[a, b]}] \\
	& \leq \frac{1}{b - a} \sup_n \mathbb{E}[(X_n - a)^{+}] \\
	& \leq \frac{1}{b - a} \left( \sup_n \mathbb{E}[X_n^{+}] + |a| \right) \leq \frac{c}{b-a} < \infty.
	\end{align*}
Donde $c$ es alguna constante y es finita por las condiciones estipuladas. Más aún, como $\mathbb{E}[U(a, b)] < \infty$ entonces $\mathbb{P}(U(a, b) < \infty) = 1$, es decir, el proceso $X_n$ realiza un número finito de cruces ascendentes casi seguramente.

Veamos que el conjunto de puntos muestrales donde el límite de $X_n$ no existe tiene probabilidad cero. Para esto, consideremos para todo $a < b$
	\begin{align*}
	\Lambda_{a, b} = \left\{ \omega : \liminf_n X_n(\omega) \leq a < b \leq  \limsup_n X_n(\omega) \right\}
	\end{align*}
Como $X_n$ realiza un número finito de cruces ascendentes entonces $\mathbb{P}(\Lambda_{a, b}) = 0$, además si $\Lambda = \bigcup_{a, b \in \mathbb{Q}} \Lambda_{a, b}$ entonces $\mathbb{P}(\Lambda) = 0$. \\

Por otro lado, tenemos que 
	\begin{align*}
	\Lambda & = \bigcup_{{a, b} \in \mathbb{Q}} \Lambda_{a, b} \\
	& = \bigcup_{{a, b} \in \mathbb{Q}} \left\{ \omega : \liminf_n X_n(\omega) \leq a < b \leq  \limsup_n X_n(\omega) \right\} \\
	& = \left\{ \omega : \liminf_n X_n(\omega) < \limsup_n X_n(\omega) \right\}.
	\end{align*}
Por lo tanto, $\mathbb{P}(\liminf_n X_n < \limsup_n X_n ) = 0$. Entonces $\mathbb{P}(\liminf_n X_n = \limsup_n X_n \}) = 1$ concluyendo que $\lim_n X_n$ existe casi seguramente. \\

Veamos que $X = \lim_n X_n \in L^1$. Como $X_n$ es un submartingala entonces tenemos que $\mathbb{E}[X_0] \leq \mathbb{E}[X_n]$, por lo tanto
	\begin{align}
	\mathbb{E}[|X_n|] & = \mathbb{E}[X_n^{+}] + \mathbb{E}[X_n^{-}] \nonumber \\
	& = 2\mathbb{E}[X_n^{+}] - \mathbb{E}[X_n] \nonumber \\
	& \leq 2\mathbb{E}[X_n^{+}] - \mathbb{E}[X_0]. \label{aar}
	\end{align}
Entonces usando el Lema de Fatou \cite[p.~205]{jacodprotter} y nuestra desigualdad (\ref{aar}), además de que por hipótesis $\sup_n \mathbb{E}[X_n^{+}] < \infty$,
	\begin{align*}
	\mathbb{E}\left[\lim_n |X_n|\right] \leq \liminf_{n \rightarrow \infty} \mathbb{E}[|X_n|] \leq 2 \sup_n \mathbb{E}[X_n^{+}] - \mathbb{E}[X_0] < \infty,
	\end{align*}
lo que implica que $X_n$ converge a un límite $X$ finito y además $\mathbb{E}[X] < \infty$, es decir $X \in L^1$.
\end{proof}

\begin{corollary}
Si $X_n$ es una supermartingala no negativa, o una martingala acotada por arriba o acotada por debajo, entonces $\lim_{n \rightarrow \infty} X_n = X$ existe c.s., y $X \in L^1$.
\end{corollary}
\begin{proof}
Si $X_n$ es una supermartingala no negativa tenemos que $(-X_n, n \geq 0)$ es una submartingala acotada por arriba por 0 y se aplica el Teorema \ref{conver1}. \\

Si $(X_n, n \geq 0)$ es una martingala acotada por debajo, entonces $X_n \geq -c$ casi seguramente, para toda $n$ y alguna constante $c > 0$. Definamos $Y_n = X_n + c$, entonces, $Y_n$ es una martingala no negativa y resulta ser una supermartingala no negativa, por lo que tenemos el caso anterior, donde aplicamos de nuevo el Teorema \ref{conver1}. \\

Si $(X_n, n \geq 0)$ es una martingala acotada por arriba, se tiene que $(-X_n, n \geq 0)$ es una martingala acotada por debajo.
\end{proof}

En general la convergencia $L^1$ del proceso $X_n$ a $X$ no se da. Para conseguir ese tipo de convergencia necesitamos hipótesis más fuertes, por lo que debemos introducir una nueva definición.

\begin{definition}
Un conjunto $\mathcal{H}$ de $L^1$ se dice que es una colección de variables aleatorias uniformemente integrables si
	\begin{align*}
	\lim_{c \rightarrow \infty} \sup_{X \in \mathcal{H}} \mathbb{E}[|X| 1_{ \{|X| \geq c\} }] = 0.
	\end{align*}
\end{definition}

Con los siguientes resultados podemos asegurar la condición de integrabilidad a partir de dos condiciones.

\begin{proposition}
Sea $\mathcal{H}$ una clase de variables aleatorias
	\begin{enumerate}
	\item Si $\sup_{X \in \mathcal{H}} \mathbb{E}[|X|^p] < \infty$ para alguna $p > 1$, entonces $\mathcal{H}$ es uniformemente integrable.
	\item Si existe una variable aleatoria $Y$ tal que $|X| < Y$ casi seguramente para toda $X \in \mathcal{H}$ y $\mathbb{E}[Y] < \infty$, entonces $\mathcal{H}$ es uniformemente integrable.
	\end{enumerate}
\end{proposition}

\begin{proof}
	\begin{enumerate}
	\item Como $\sup_{X \in \mathcal{H}} \mathbb{E}[|X|^p] < \infty$, consideremos a $K$ como la constante tal que $\sup_{X \in \mathcal{H}} \mathbb{E}[|X|^p] < K < \infty$. Si $0 < c \leq x$, entonces como $p > 1$ se tiene que $x^{1-p} \leq c^{1-p}$, si multiplicamos por $x^p$ obtenemos $x \leq c^{1-p}x^p$, por lo tanto
		\begin{align*}
		|X| 1_{\{|X| > c\}} \leq c^{1-p}  |X|^p 1_{\{|X| > c\}}.
		\end{align*}
	Y entonces,
		\begin{align*}
		\mathbb{E}\left[ |X| 1_{\{|X| > c\}} \right] \leq c^{1-p}  \mathbb{E} \left[ |X|^p 1_{\{|X| > c\}} \right] \leq \frac{k}{c^{p-1}},
		\end{align*}
	si tomamos el supremo sobre toda $X \in \mathcal{H}$ y hacemos que $n \rightarrow \infty$, tenemos que
		\begin{align*}
		\lim_{c \rightarrow \infty} \sup_{X \in \mathcal{H}} \mathbb{E} \left[ |X| 1_{ \{|X| > c\} } \right] \leq \lim_{c \rightarrow \infty} \frac{k}{c^{p-1}} = 0.
		\end{align*}
		
	\item Como $|X| \leq Y$ casi seguramente, para toda $X \in \mathcal{H}$, se tiene
		\begin{align*}
		|X| 1_{ \{|X| > c\} } \leq Y 1_{ \{Y > c\} }.
		\end{align*}
	Tomando el supremo sobre toda $X \in \mathcal{H}$ y el límite en ambos lados de la desigualdad tenemos
		\begin{align*}
		\lim_{c \rightarrow \infty} \sup_{X \in \mathcal{H}} \mathbb{E} \left[ |X| 1_{ \{|X| > c\} } \right] \leq \lim_{c \rightarrow \infty} \mathbb{E}\left[Y 1_{ \{Y > c\} }\right].
		\end{align*}
	Por otro lado, tenemos que $\lim_{c \rightarrow \infty} Y 1_{ \{Y > c\} } = 0$ c.s. y por el Teorema de Convergencia Dominada de Lebesgue obtenemos
		\begin{align*}
		\lim_{c \rightarrow \infty} \sup_{X \in \mathcal{H}} \mathbb{E} \left[ |X| 1_{ \{|X| > c\} } \right] & \leq \lim_{c \rightarrow \infty} \mathbb{E} \left[ Y 1_{ \{Y > c\} } \right] = \mathbb{E} \left[ \lim_{c \rightarrow \infty} Y 1_{ \{Y > c\} } \right] = 0.
		\end{align*}
	\end{enumerate}
\end{proof}

El siguiente resultado es una versión más fuerte del Teorema \ref{conver1} para el caso de martingalas.

\begin{theorem}
	\begin{enumerate}
	\item Sea $(M_n, n \geq 0)$ una martingala y supongamos que $(M_n, n \geq 0)$ es una colección de variables aleatorias uniformemente integrables. Entonces
	\begin{align*}
	\lim_{n \rightarrow \infty} M_n = M_{\infty} \text{ existe c.s. }
	\end{align*}
	$M_{\infty}$ está en $L^1$, y $M_n$ converge a $M_{\infty}$ en $L^1$. Más aún, la propiedad de martingala se mantiene para $M_{\infty}$, es decir, $M_n = \mathbb{E} \left[ M_{\infty} \mid \mathcal{F}_n \right]$.
	
	\item Sea $Y \in L^1$ y considere la martingala $M_n = \mathbb{E} \left[ Y \mid \mathcal{F}_n \right]$. Entonces $(M_n, n \geq 1)$ es una colección de variables aleatorias uniformemente integrables.
	\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item De la definición de uniformemente integrable para variables aleatorias, tenemos que, para toda $\epsilon > 0$ existe una constante $c > 0$ tal que $\sup_{X \in \mathcal{H}} \mathbb{E} [ 1_{ \{|X| > c\} } |X| ] \leq \epsilon$. Entonces
	\begin{align*}
	\mathbb{E}[|M_n|] = \mathbb{E} \left[ |M_n|1_{ \{|M_n| > c\} } \right] + \mathbb{E} \left[ |M_n|1_{ \{|M_n| \leq c\} } \right] \leq \epsilon + c.
	\end{align*}
Por lo tanto $(M_n)_{n \geq 0}$ está en acotado en $L^1$, es por esta razón que se tiene $\sup_n \mathbb{E}[M_n^{+}] < \infty$. Utilizando el Teorema \ref{conver1} 
	\begin{align*}
	\lim_{n \rightarrow \infty} M_n = M_{\infty} \text{ existe c.s. y } M_{\infty} \text{ está en } L^1.
	\end{align*}
Para mostrar la convergencia del proceso a la variables aleatoria $M_{\infty}$ en $L^1$ veamos que para una constante $N$ suficientemente grande $\mathbb{E}[|M_n - M_{\infty}|] < \epsilon$ para toda $n \geq N$. \\

Recordemos que una función cumple la condición de Lipschitz si \cite[p.~169]{bartle} dados dos espacios métricos $(X, d_X)$ y $(Y, d_Y)$ existe una constante $K$ tal que
	\begin{align*}
	\frac{d_Y(f(x) - f(y))}{d_X(x - y)} \leq K.
	\end{align*}
Si definimos 
	\begin{align*}
	f_c(x) = 
	\begin{cases}
	c, & \text{ si } c > x; \\
	x, & \text{ si } |x| \leq c; \\
	-c, & \text{ si } x < -c.
	\end{cases}
	\end{align*}
Tenemos que $f_c(x)$ es Lipschitz. Por la integrabilidad uniforme sabemos que existe $c$ suficientemente grande tal que para cualquier $\epsilon > 0$ dada:
	\begin{align}
	\mathbb{E}[ |f_c(M_n) & - M_n|] < \frac{\epsilon}{3}, \text{ para toda } n; \label{aas} \\
	\mathbb{E}[ |f_c(M_{\infty}) & - M_{\infty}|] < \frac{\epsilon}{3}. \label{aat}
	\end{align}
	
Como $\lim_n M_n = M_{\infty}$ y por la condición de $f_c$ tenemos que $\lim_n f_c(M_n) = f_c(M_{\infty})$. Por el Teorema de Convergencia Dominada de Lebesgue \cite[p.~52]{jacodprotter} tenemos que para cualquier $n \geq N$, para $N$ suficientemente grande
	\begin{align}
	\mathbb{E} [ | f_c(M_n) & - f_c(M_{\infty}) |] < \frac{\epsilon}{3}. \label{aau}
	\end{align}

Combinando los resultados (\ref{aas}), (\ref{aat}) y (\ref{aau}) obtenemos la desigualdad requerida para asegurar que $M_n \xrightarrow{L^1} M_{\infty}$. Para demostrar que $\mathbb{E}\left[M_{\infty} \mid \mathcal{F}_n\right] = M_n$ c.s. veamos que para cualquier conjunto $\mathcal{F}_n$-medible se tiene $\mathbb{E}[M_n 1_{\Lambda}] = \mathbb{E}[M_{\infty} 1_{\Lambda}]$. Consideremos $\Lambda \in \mathcal{F}_m$ y $n \geq m$. Entonces por la propiedad de martingala tenemos que
	\begin{align*}
	\mathbb{E}[M_n 1_{\Lambda}] = \mathbb{E}[M_m 1_{\Lambda}].
	\end{align*}
Sin embargo, 
	\begin{align*}
	|\mathbb{E}[M_m 1_{\Lambda}] - \mathbb{E}[M_{\infty} 1_{\Lambda}]| & \leq |\mathbb{E}[(M_m - M_{\infty}) 1_{\Lambda}]| \\
	& \leq \mathbb{E}[|(M_m - M_{\infty}) 1_{\Lambda}|] \\
	& \leq \mathbb{E}[|M_m - M_{\infty}|],
	\end{align*}
Donde $\mathbb{E}[|M_n - M_{\infty}|] \rightarrow 0$ cuando $n \rightarrow 0$, por lo tanto, $\mathbb{E}[M_{\infty} \mid \mathcal{F}_m] = M_m$ c.s.

\item Sabemos $(M_n, n \geq 0)$ es una martingala. Para una variable $Y \in L^1$, con $c > 0$ tenemos que
	\begin{align*}
	M_n 1_{\{|M_n > c|\}} = \mathbb{E}\left[Y1_{\{|M_n > c|\}} \mid \mathcal{F}_n\right],
	\end{align*}
ya que el evento $\{|M_n| > c\}$ pertenece a $\mathcal{F}_n$. Por lo tanto, para cualquier constante $d > 0$ se tiene
	\begin{align*}
	\mathbb{E}\left[|M_n| 1_{\{|M_n > c|\}}\right] & = \mathbb{E}\left[|Y|1_{\{|M_n > c|\}}\right] \\
	& \leq \mathbb{E}\left[|Y|1_{\{|Y > d|\}} \right] + d\mathbb{P}(|M_n| > c) \\
	& \leq \mathbb{E}\left[|Y|1_{\{|Y > d|\}} \right] + \frac{d}{c}\mathbb{E}[|M_n|].
	\end{align*}
Si tomamos un $\epsilon$ arbitrario y escogemos a la constante $d$ tal que 
	\begin{align*}
	\mathbb{E}\left[|Y|1_{\{|Y > d|\}}\right] < \frac{\epsilon}{2}.
	\end{align*}
Y $c$ como la constante tal que 
	\begin{align*}
	\frac{d}{c} \mathbb{E}[|M_n|] < \frac{\epsilon}{2},
	\end{align*}
	entonces tenemos que $\mathbb{E}[|M_n| 1_{\{|M_n > c|\}}] < \epsilon$ para todo $n$.
\end{enumerate}
\end{proof}

La propiedad de martingala estudiada hasta el momento contempla números enteros positivos, pero también podemos considerar el conjunto de índices $-\mathbb{N}$: los enteros negativos. Una martingala reversible es una sucesión $(X_{-n}, n \in \mathbb{N})$ de variables aleatorias integrables si, $X_{-n}$ es $\mathcal{F}_{-n}$ medible y satisface
	\begin{align}
	\mathbb{E}[X_{-n} \mid \mathcal{F}_{-m}] = M_{-m}, \hspace{0.3cm} \text{ c.s } \label{aav}
	\end{align}
donde $0 \leq n < m$.

\begin{theorem}[Teorema de Convergencia de Martingalas Reversibles]
\label{conver2}
Sea $(X_{-n}, \mathcal{F}_{-n})_{n \in \mathbb{N}}$ una martingala reversible, y sea $\mathcal{F}_{-\infty} = \cap_{n = 0}^{\infty} \mathcal{F}_{-n}$. Entonces la sequencia $(X_{-n})$ converge c.s. y en $L^1$ al límite $X$ c.s., cuando $n \rightarrow +\infty$ (en particular $X$ es c.s. finita e integrable).
\end{theorem}
\begin{proof}
Sea $U_{-n}^{[a, b]}$ es el número de cruces ascendentes de $(X_{-n}, n \geq 0)$ de $[a, b]$ entre el tiempo $-n$ y $1$. Entonces $U_{-n}^{[a, b]}$ es creciente mientras $n$ crece, además consideremos $U^{-}(a, b) = \lim_n U_{-n}^{[a, b]}$, el cual existe por ser $U_{-n}^{[a, b]}$ una sucesión creciente y acotada. Por el Teorema de Convergencia Monótona
	\begin{align*}
	\mathbb{E}[U^{-}(a, b)] & = \lim_{n \rightarrow \infty} \mathbb{E}[U_{-n}^{[a, b]}] \leq \frac{1}{b-a} \mathbb{E}[(-X_0 - a)^{+}] < \infty.
	\end{align*}
Entonces $\mathbb{P}(U^{-}(a, b) < \infty) = 1$. El mismo argumento de los cruces ascendentes que se utilizó en la prueba del Teorema \ref{conver1} implica que el $\lim_{n \rightarrow \infty} X_{-n} = X$ existe c.s. \\

Sea $\phi(x) = x^{+} = (x \vee 0)$, la cual es una función convexa y creciente, entonces $\phi(X_{-n})$ es integrable para toda $n$. De la desigualdad de Jensen y (\ref{aav}) se infiere que 
	\begin{align*}
	X_{-n}^{+} \leq \mathbb{E}[X_0^{+} \mid \mathcal{F}_{-n}].
	\end{align*}
Por lo tanto
	\begin{align*}
	\mathbb{E}[X_{-n}^{+}] \leq \mathbb{E}[X_0^{+}].
	\end{align*}
Del Lema de Fatou y por el hecho de que $X_{-n}^{+} \geq 1$ y $X_{-n}^{+} \rightarrow X^{+}$ c.s. se tiene
	\begin{align*}
	\mathbb{E}[X^{+}] \leq \liminf_n \mathbb{E}[X_{-n}^{+}] \leq \mathbb{E}[X_0^{+}] < \infty.
	\end{align*}
Implicando que $X^{+} \in L^1$ y por el mismo argumento aplicado a la martingala $(-X_n)$ se muestra que $X^{-} \in L^1$, entonces $X \in L^1$. \\

La convergencia en $L^1$ es una implicación del Teorema \ref{conver1}, pues se demostró que, si $X_{-n} \rightarrow X$ c.s., si $X \in L^1$ y además $(X_{-n})$ es uniformemente integrable, entonces $X_{-n} \rightarrow X$ en $L^1$.
\end{proof}

Los Teoremas de Convergencia de Martingalas probados hasta ahora (Teoremas \ref{conver1} y \ref{conver2}) son resultados de convergencia fuerte: todas las variables aleatorias están definidas en el mismo espacio de probabilidad y convergen fuertemente a variables aleatorias del mismo espacio, casi seguramente y en $L^1$. \\ 

Es posible desarrollar un resultado de convergencia débil, para una clase de martingalas que no satisfacen las condiciones mencionadas en el Teorema \ref{conver1}. El límite resulta una distribución normal  y tal teorema es conocido como \emph{Teorema del Límite Central para Martingalas}.

\begin{theorem}[Teorema del Límite Central para Martingalas]
\label{conver3}
Sea $(X_n, n \geq 1)$ una sucesión de variables aleatorias que satisface
	\begin{enumerate}
	\item $\mathbb{E}[X_n \mid \mathcal{F}_{n-1}] = 0$.
	\item $\mathbb{E}[X_n^2 \mid \mathcal{F}_{n-1}] = 1$.
	\item $\mathbb{E}[|X_n|^3 \mid \mathcal{F}_{n-1}] \leq K < \infty$.
	\end{enumerate}
Sea $S_n = \sum_{i=1}^n X_i$ con $S_0 = 0$. Entonces $\lim_{n \rightarrow \infty} \frac{1}{\sqrt{n}} S_n = Z$, donde $Z$ es una variable aleatoria con distribución $N(0, 1)$, además la convergencia es en distribución.
\end{theorem}
\begin{proof}
Para verificar la convergencia en distribución de la proposición haremos uso de las funciones características. Para $u \in \mathbb{R}$, recordemos que $\varphi_X (u) = \mathbb{E}[e^{iuX}]$ es la función característica de $X$. Definamos la siguiente función  como
	\begin{align*}
	\varphi_{n, j} (u) = \mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} X_j} \bigg| \mathcal{F}_{j-1}\right].
	\end{align*}
Por el Teorema de Taylor \cite[p.~358]{apostol} tenemos
	\begin{align}
	e^{iu \frac{1}{\sqrt{n}} X_j} = 1 + iu \frac{1}{\sqrt{n}} X_j - \frac{u^2}{2n} X_j^2 - \frac{iu^3}{6n^{\frac{3}{2}}} \bar{X}_j^3, \label{aaw}
	\end{align}
donde $\bar{X}_j$ es un valor (aleatorio) que se encuentra entre $1$ y $X_j$. Al tomar la esperanza condicional de ambos lados de (\ref{aaw}) obtenemos
	\begin{align*}
	\varphi_{n, j} (u) = 1 + iu \frac{1}{\sqrt{n}} \mathbb{E}[X_j \mid \mathcal{F}_{j-1}] - \frac{u^2}{2n} \mathbb{E}[X_j^2 \mid \mathcal{F}_{j-1}] - \frac{iu^3}{6n^{\frac{3}{2}}} \mathbb{E}[\bar{X}_j^3 \mid \mathcal{F}_{j-1}].
	\end{align*}
Haciendo uso de las hipótesis (1) y (2) tenemos,
	\begin{align}
	\varphi_{n, j} (u) = 1 - \frac{u^2}{2n} - \frac{iu^3}{6n^{\frac{3}{2}}} \mathbb{E}[\bar{X}_j^3 \mid \mathcal{F}_{j-1}]. \label{aax}
	\end{align}
Además como $|\bar{X}_j| \leq |X_j|$ y $S_p = \sum_{i=1}^p X_i$, para $1 \leq p \leq n$ tenemos
	\begin{align}
	\mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_p} \right] & = \mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} (S_{p-1} + X_p) } \right] \label{aay} \\
	& = \mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_{p-1}} e^{iu \frac{1}{\sqrt{n}} X_p} \right] \nonumber \\ 
	& = \mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_{p-1}} \mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} X_p} \bigg| \mathcal{F}_{p-1} \right]\right] \nonumber \\ 
	& = \mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_{p-1}} \varphi_{n, p} (u) \right] \nonumber.
	\end{align}
Usando (\ref{aay}) y (\ref{aax}) tenemos
	\begin{align*}
	\mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_p} \right] = \mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_{p-1}} \left[ 1 - \frac{u^2}{2n} - \frac{iu^3}{6n^{\frac{3}{2}}} \mathbb{E}[\bar{X}_j^3 \mid \mathcal{F}_{j-1}] \right]\right].
	\end{align*}
Desarrollando la ecuación,
	\begin{align}
	\mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_p} - \left(1 - \frac{u^2}{2n}\right)e^{iu \frac{1}{\sqrt{n}} S_{p-1}}\right] = \mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_{p-1}} \frac{iu^3}{6n^{\frac{3}{2}}} \mathbb{E}[\bar{X}_j^3 \mid \mathcal{F}_{j-1}] \right]. \label{aaz}
	\end{align}
Tomando el módulo en ambos lados de (\ref{aaz}) y usando la hipótesis (3)
	\begin{align*}
	\left| \mathbb{E} \left[e^{iu \frac{1}{\sqrt{n}} S_p} - \left(1 - \frac{u^2}{2n}\right) e^{iu \frac{1}{\sqrt{n}} S_{p-1}} \right]\right| & \leq \mathbb{E} \left[ | e^{iu \frac{1}{\sqrt{n}} S_{p1}} | \frac{|u|^3}{6n^{\frac{3}{2}}} \mathbb{E}[| \bar{X}_j^3 | \mid \mathcal{F}_{j-1}] \right] \\
	& \leq K \frac{|u|^3}{6n^{\frac{3}{2}}}.
	\end{align*} 
Si fijamos el valor de $u \in \mathbb{R}$ y hacemos tender a $n \rightarrow \infty$, eventualmente $n \geq u^2/2$ y es por esto que para una $n$ suficientemente grande tenemos que $0 \leq 1 - \frac{u^2}{2} \leq 1$. Si multiplicamos el lado izquierdo de desigualdad anterior por $(1 - u^2/2)^{n-p}$ para una $n$ suficientemente grande
	\begin{align*}
	\left| \left(1 - \frac{u^2}{2}\right)^{n-p} \mathbb{E} \left[e^{iu \frac{1}{\sqrt{n}} S_p} - \left(1 - \frac{u^2}{2n}\right)^{n-p+1}\right]\mathbb{E}\left[e^{iu \frac{1}{\sqrt{n}} S_{p-1}} \right] \right| \leq K \frac{|u|^3}{6n^{\frac{3}{2}}}.
	\end{align*}
Para finalizar, al usar la propiedad telescópica de sumas (finitas) observamos que 
	\begin{align*}
	& \mathbb{E} \left[e^{iu \frac{1}{\sqrt{n}} S_n} \right] - \left(1 - \frac{u^2}{2n}\right)^{n} \\ 
	& = \sum_{p=1}^{n} \left(1 - \frac{u^2}{2n}\right)^{n-p} \mathbb{E} \left[e^{iu \frac{1}{\sqrt{n}} S_p} \right] - \left(1 - \frac{u^2}{2n}\right)^{n-(p-1)} \mathbb{E} \left[e^{iu \frac{1}{\sqrt{n}} S_{p-1}} \right].
	\end{align*}
Usando la desigualdad del triángulo y de la última desigualdad tenemos que (siempre para $n \geq 2/u^2$)
	\begin{align}
	\left| \mathbb{E} \left[e^{iu \frac{1}{\sqrt{n}} S_n} \right] - \left(1 - \frac{u^2}{2n}\right)^{n} \right| \leq n K \frac{|u|^3}{6n^{\frac{3}{2}}} = K \frac{|u|^3}{6\sqrt{n}}. \label{aba}
	\end{align}
Como el lado derecho de (\ref{aba}) tiende a $0$ y además
	\begin{align}
	\lim_{n \rightarrow \infty} \left(1 - \frac{u^2}{2n}\right)^{n} = e^{-\frac{u^2}{2}}.
	\end{align}
Usando la regla de L'Hôpital \cite[p.~215]{bartle}, tenemos
	\begin{align}
	\lim_{n \rightarrow \infty}\mathbb{E} \left[e^{iu \frac{S_n}{\sqrt{n}}} \right] = e^{-\frac{u^2}{2}}.
	\end{align}

Por el Teorema de Continuidad de Lévy \cite[p.~166]{jacodprotter}, el cual nos permite relacionar la convergencia puntal (de las funciones características) con la convergencia en distribución, tenemos que $S_n/\sqrt{n}$ converge en distribución  $Z$, donde la función característica de $Z$ es $e^{-(u^2)/2}$, la cual es una función característica de una variable aleatoria con distribución $N(0, 1)$.
\end{proof}

La proposición anterior se establece de forma similar en el Teorema del Límite Central \cite[p.~181]{jacodprotter} para variables independientes e idénticamente distribuidas $X_n$, con sumas parciales $S_n$. La condición $(1)$ implica que $(S_n)$ es una martingala, pues $X_n = S_n - S_{n-1}$. 

% Por otro lado, un martingala arbitraria $(S_n)$ es una sucesión de sumas parciales asociada con las variables aleatorias $X_n = S_n - S_{n-1}$ y esto también satisface $(1)$.

% Si $S_n$ es la martingala del Teorema \ref{conver1}, sabemos que no puede existir una convergencia fuerte ya que si tenemos $\lim_n S_n = S$ c.s. con $S$ en $L^1$ entonces podríamos tener $\lim_n S_n/\sqrt{n} = 0$ c.s., y la convergencia débil de $S_n/\sqrt{n}$ a una variable aleatoria normal no sería posible. \\

% Lo que no hace posible tener una convergencia fuerte en martingala es el comportamiento de las varianzas condicionales de los incrementos de la martingala $X_n$ (hipótesis (2) del Teorema \ref{conver3}).


